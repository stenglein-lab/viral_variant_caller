params {

  // ------------------------
  // input directory defaults
  // ------------------------
  input_dir   = "$baseDir/input/"
  fastq_dir   = "${input_dir}/fastq/"
  refseq_dir  = "${input_dir}/refseq/"
  
  // a regex pattern that will be matched for input fastq
  fastq_pattern = "*_R{1,2}*.fastq*"
  
  // -------------------------
  // output directory defaults
  // -------------------------
  outdir               = "$baseDir/results"                                                       
  initial_fastqc_dir   = "${outdir}/initial_fastqc/" 
  post_trim_fastqc_dir = "${outdir}/post_trim_fastqc/" 
  counts_out_dir       = "${outdir}/fastq_counts/"                        
  fastq_out_dir        = "${outdir}/trimmed_fastq/"                        
  bam_out_dir          = "${outdir}/bam/"                                    
  vcf_out_dir          = "${outdir}/vcf/"  
  consensus_out_dir    = "${outdir}/consensus_sequences/"  
  consensus_pass_dir   = "${consensus_out_dir}/sufficiently_complete/"  
  consensus_fail_dir   = "${consensus_out_dir}/insufficiently_complete/"  
  pangolin_out_dir     = "${outdir}/pangolin/"
  pangolin_datadir     = "$HOME/pangolin_data"
  ditector_out_dir     = "${outdir}/ditector/"  
  tracedir             = "${outdir}/pipeline_info"

  // prefix for pipeline output files: today's date.  
  // can override this (--output_file_prefix whatever) on command line
  output_prefix        = new java.util.Date().format( 'yyyy_MM_dd_')
  
  // ---------------------------
  // Scripts directory defaults
  // ---------------------------
  script_dir           = "${baseDir}/scripts"
  
  // -----------------------------
  // Host cell filtering defaults
  // -----------------------------
  
  // Human samples: use human genome for host filtering
  host_bt_index      = "/home/databases/human/GCRh38"
  host_bt_suffix     = "human_genome"
  host_bt_min_score  = "60"
  host_bt_threads    = "8"
  
  // -------------------
  // Reference sequence
  // -------------------
  // SARS-CoV-2 Wuhan-1 (NC_045512) reference sequence, 
  // or a reference seq of your choosing
  refseq_name          = "NC_045512"
  refseq_fasta         = "${refseq_dir}/${refseq_name}.fasta"
  refseq_genbank       = "${refseq_dir}/${refseq_name}.gb"
  refseq_bt_index      = "${refseq_dir}/${refseq_name}"
  refseq_bt_min_score  = "120"
  refseq_bwa_threads   = "8"
  
  // ------------------
  // Trimming settings
  // ------------------
  always_trim_5p_bases  = "0" 
  always_trim_3p_bases  = "0" 
  post_trim_min_length  = "30" 
  bwa_clipping_penalty  = "1"
  
  // -----------------------------------------
  // Fasta format files of primers to trim off
  // -----------------------------------------
  primer_fasta_5p  = "${refseq_dir}/swift_primers.5p.fasta"
  primer_fasta_3p  = "${refseq_dir}/swift_primers.3p.fasta"
  primer_bed       = "${refseq_dir}/swift_v2_primers.bed"
  ivar_trim        = true
  ivar_trim_offset = "5"

  // -------------------------
  // Variant calling defaults
  // -------------------------

  // Call and annotate intra-host variants?
  call_variants   = false

  // min depth and allele freq for calling variants and indels                    
  min_depth_for_variant_call = "40"
  min_allele_freq            = "0.03"
  
  // DI-tector info
  ditector_script="${script_dir}/DI-tector_06.py"
  // to run ditector, both call_variants and run_ditector have to be true
  run_ditector    = false
  
  // conda for snpEFF
  snpeff_cfg      = "${refseq_dir}/snpEff.config" 
  snpeff_data     = "${refseq_dir}/snpeff_data/"
  snpeff_threads  = "8"
  
  // regions to omit from BSQR step
  ignore_regions  = "${refseq_dir}/ignore_regions.bed"
  
  // optional custom variant annotation
  custom_annotations         = false
  custom_annotations_bed     = "${refseq_dir}/voc_positions.bed"

  // ----------------------------------------
  // Submission of sequence data to databases
  // ----------------------------------------

  // minimum fraction called (fraction non-N bases in consensus sequence)
  // to consider sequence acceptable for submission
  minimum_fraction_called = 0.95

  prepare_gisaid           = true
  gisaid_metadata_file     = "${input_dir}/gisaid_metadata.tsv"

  // an [optional] file containing an encryption key
  // for use in obfuscating sample IDs 
  key_file                   = "${input_dir}/.key.txt"
  
  // -----------------------
  // Miscellaneous settings
  // -----------------------
  
  // plot mapping stats
  plot_mapping_stats  = false
  
  // flag to optionally run cd-hit to collapse non-unique reads
  // skip this step by default
  skip_collapse_to_unique    = true
  
  // cd-hit-dup cutoff for collapsing reads with >= this much fractional similarity
  duplicate_cutoff           =  "0.98"

  singularity_pull_docker_container = false

}


process {

  withLabel: 'highmem' {
    maxForks = 4
	 cpus = 12
  }

  withLabel: 'lowmem_threaded' {
    maxForks = 6
	 cpus = 8
  }

  withLabel: 'lowmem_non_threaded' {
    maxForks = 24
	 cpus = 1
  }
}

profiles {

  local {
    exector.name           = 'local'
    executor.queueSize     = 48
    executor.cpus          = 48
    executor.memory        = '256 GB'
  }

  conda {
    params.enable_conda    = true
    process.conda          = "./environment_setup/variant_conda_environment.yaml"
    conda.cacheDir         = "$HOME/conda_cacheDir"
    conda.createTimeout    = '1 h'
    singularity.enabled    = false
  }

  /*
   -----------------------------------------
   Server specific singularity mount points 
   -----------------------------------------

   Singularity by default mounts certain directories, including a user's home directory
   See https://sylabs.io/guides/latest/user-guide/bind_paths_and_mounts.html#system-defined-bind-paths
  
   You may have to mount additional directories that singularity-run processes
   will need access to.  For example:
  
   1.  If you will be running the pipeline from a directory outside your 
   user's home directory, you will have specify the base directory's location 
   
   2. You might have to mount directories containing shared databases 
      such as the /home/databases/human example below

   TODO: better document this issue in pipeline documentation

   The singularity options below specify server-specific directories to mount
  */

  cctsi {
    singularity.runOptions = "-B /home/cdphe_sequencing -B /home/databases/human/"
  }

  aidlngs {
    singularity.runOptions = "-B /home/databases/human/"
  }

  singularity {
    params.enable_conda    = false
    singularity.enabled    = true
    singularity.autoMounts = true
    singularity.cacheDir   = "$HOME/singularity_cacheDir"
  }

  // TODO: implement testing
  test {
    includeConfig 'conf/test.config'
  }
}


def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.pdf"
}

manifest {
    name            = 'stenglein-lab/viral_variant_caller'
    author          = 'Mark Stenglein'
    homePage        = 'https://github.com/stenglein-lab/viral_variant_caller'
    description     = 'A pipeline to call viral variants and calculate consensus sequences'
    mainScript      = 'main.nf'
    nextflowVersion = '!>=21.04.0'
    version         = '1.0'
}



// Turn this option on to delete all intermediate files from the analysis
// see: https://www.nextflow.io/docs/latest/config.html
// cleanup = true
